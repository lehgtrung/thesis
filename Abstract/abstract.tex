\newpage
\chapter*{TÓM TẮT}
\addcontentsline{toc}{chapter}{TÓM TẮT} 

 Hiện nay, trong xu hướng toàn cầu hóa, nhu cầu đọc hiểu văn bản tiếng nước ngoài hay giao tiếp giữa những người không cùng ngôn ngữ là một nhu cầu thiết yếu. Do đó cần một công cụ có khả năng chuyển đổi giữa các ngôn ngữ mà không cần sự trợ giúp của con người. Công cụ như vậy được gọi là dịch máy. Nhờ vào những tiến bộ trong học máy, đặc biệt là học sâu mà bài toán dịch máy đã trở nên đơn giản và hiệu quả hơn. Tuy nhiên nó vẫn chứa nhiều thách thức và là một bài toán lớn thu hút nhiều sự chú ý trong cộng đồng xử lý ngôn ngữ tự nhiên nói riêng và học máy nói chung.
 
 Trong khóa luận này, nhóm chúng em tập trung tìm hiểu về mô hình dịch máy LSTM-Attention được đề xuất trong bài báo\textit{ “Effective Approaches to Attention-based Neural Machine Translation”} đăng trong hội nghị EMNLP 2015 của nhóm tác giả đại học Stanford (Minh Thang Luong, Hieu Pham, Chris D. Manning). Trong bài báo này, nhóm tác giả đã đề xuất một mô hình dịch máy dựa trên mạng LSTM và cơ chế Attention, đây là mô hình dịch máy đạt kết quả cao nhất tại thời điểm ra mắt (2015). Mô hình này là một mạng LSTM nhiều tầng kết hợp với cơ chế Attention, mô hình có khả năng nhận vào câu đầu vào trong ngôn ngữ nguồn và phát sinh ra câu đầu ra trong ngôn ngữ đích. 
 
 Chúng em đã thành công xây dựng lại mô hình mà nhóm tác giả đã đề xuất trên hai tập dữ liệu Anh-Đức và Anh-Việt, đồng thời thực hiện những thí nghiệm để chứng minh độ hiệu quả của mô hình cũng như so sánh kết quả của nhóm và kết quả của bài báo. Thực nghiệm cho thấy, độ chính xác mà mô hình của nhóm đạt được xấp xỉ độ chính xác của tác giả, những chênh lệch trong độ chính xác được giải thích là do sự khác biệt về nền tảng, thư viện dùng cho thực nghiệm.
 