% \pagebreak[4]
% \hspace*{1cm}
% \pagebreak[4]
% \hspace*{1cm}
% \pagebreak[4]

\chapter{Giới Thiệu }
\ifpdf
    \graphicspath{{Chapter1/Chapter1Figs/PNG/}{Chapter1/Chapter1Figs/PDF/}{Chapter1/Chapter1Figs/}}
\else
    \graphicspath{{Chapter1/Chapter1Figs/EPS/}{Chapter1/Chapter1Figs/}}
\fi

%\begin{quote}
%\textit{Nhận dạng hành động người là một trong những hướng nghiên cứu thách thức nhất trong lĩnh vực thị giác máy tính. Trong chương này khóa luận tập trung giới thiệu tổng quan về bài toán, làm sáng tỏ động lực nghiên cứu cũng như các thách thức còn tồn đọng. Cuối cùng, bố cục nội dung trình bày trong khóa luận sẽ được tóm tắt ở phần kết của chương.}
%\end{quote}

Nhờ vào những cải cách trong giao thông và cơ sở hạ tầng viễn thông mà giờ đây toàn cầu hóa đang trở nên gần với chúng ta hơn bao giờ hết. Trong xu hướng đó nhu cầu giao tiếp và thông hiểu giữa những nền văn hóa là không thể thiếu. Tuy nhiên, những nền văn hóa khác nhau thường kèm theo đó là sự khác biệt về ngôn ngữ, là một trong những trở ngại lớn nhất của sự giao tiếp. Một người phải mất rất nhiều thời gian để thành thạo một ngôn ngữ không phải là tiếng mẹ đẻ và không thể nào học được nhiều ngôn ngữ cùng lúc. Cho nên, việc phát triển một công cụ để giải quyết vấn đề này là tất yếu. Một trong những công cụ như vậy là Dịch máy.

\textit{Dịch máy} là quá trình chuyển đổi văn bản/tiếng nói từ ngôn ngữ này sang dạng tương ứng của nó trong một ngôn ngữ khác được thực hiện bởi một chương trình máy tính nhằm mục đích cung cấp bản dịch tốt nhất mà không cần sự trợ giúp của con người. Dịch máy có một quá trình lịch sử lâu dài, từ thế kỷ 17, đã có những ý tưởng về một loại ngôn ngữ mang ý nghĩa phổ quát nhưng mãi đến những năm 1950 những nghiên cứu về dịch máy mới thật sự bắt đầu. Trong thời kì Chiến tranh Lạnh, vào ngày 7 tháng 1 năm 1954, tại trụ sở chính của IBM ở New York, thử nghiệm Georgetown-IBM được tiến hành. Máy tính IBM 701 đã tự động dịch 49 câu tiếng Nga sang tiếng Anh lần đầu tiên trong lịch sử chỉ sử dụng 250 từ vựng và sáu luật ngữ pháp. Thí nghiệm này được xem như là một thành công và mở ra kỉ nguyên cho những nghiên cứu với kinh phí lớn về dịch máy ở Hoa Kỳ. Ở Liên Xô những thí nghiệm tương tự cũng được thực hiện không lâu sau đó.

Trong một thập kỷ tiếp theo, nhiều nhóm nghiên cứu về dịch máy được thành lập. Một số nhóm chấp nhận phương pháp thử và sai, thường dựa trên thống kê với mục tiêu là một hệ thống dịch máy có thể hoạt động ngay lập tức, tiêu biểu như: nhóm nghiên cứu tại đại học Washington (và sau này là IBM) với hệ thống dịch Nga-Anh cho Không quân Hoa Kỳ, những nghiên cứu tại viện Cơ học Chính xác ở Liên Xô và Phòng thí nghiệm Vật lý Quốc gia ở Anh. Trong khi một số khác hướng đến giải pháp lâu dài với hướng tiếp cận lý thuyết bao gồm cả những vấn đề liên quan đến ngôn ngữ cơ bản như nhóm nghiên cứu tại Trung tâm nghiên cứu lý thuyết tại MIT, Đại học Havard và Đơn vị nghiên cứu ngôn ngữ Đại học Cambridge. Những nghiên cứu trong giai đoạn này có tầm quan trọng và ảnh hưởng lâu dài không chỉ cho Dịch máy mà còn cho nhiều ngành khác như Ngôn ngữ học tính toán, Trí tuệ nhân tạo - cụ thể là việc phát triển các từ điển tự động và kỹ thuật phân tích cú pháp. Nhiều nhóm nghiên cứu đã đóng góp đáng kể cho việc phát triển lý thuyết ngôn ngữ. Tuy nhiên, mục tiêu cơ bản của Dịch máy là xây dựng hệ thống có khả năng tạo ra bản dịch tốt lại không đạt được dẫn đến một kết quả là vào năm 1966 bản báo cáo từ Ủy ban tư vấn xử lý ngôn ngữ tự động (Automatic Language Processing Advisory) của Hoa Kỳ, tuyên bố rằng Dịch máy là đắt tiền, không chính xác và không mang lại kết quả hứa hẹn. Thay vào đó, họ đề nghị tập trung vào phát triển các từ điển, điều này đã loại bỏ các nhà nghiên cứu Mỹ ra khỏi cuộc đua trong gần một thập kỷ. 

%Từ đó đến nay, đã có nhiều hướng tiếp cập đã được sử dụng trong dịch máy với mục tiêu tạo ra bản dịch có độ chính xác cao và giảm thiểu công sức của con người đặc biệt phải kể đến Dịch máy dựa trên luật, Dịch máy thống kê hay gần đây nhất là Dịch máy Nơ-ron (hình 1.1).

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{mthistory}
	\caption[Lịch sử tóm tắt của Dịch máy]{Lịch sử tóm tắt của Dịch máy, nguồn ảnh: Ilya Pestov trong blog \href{https://medium.freecodecamp.org/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5}{A history of machine translation from the Cold War to deep learning}}
	\label{fig_mthistory}
\end{figure}

%Trong những năm tiếp theo, Dịch máy đã trải qua nhiều giai đoạn phát triển lớn nhưng cũng gặp phải một số trì trệ, đáng kể nhất là vào năm 1966 bản báo cáo từ ủy ban ALPAC (Automatic Language Processing Advisory) của Hoa Kỳ, tuyên bố rằng Dịch máy là đắt tiền, không chính xác và không mang lại kết quả hứa hẹn. Thay vào đó, họ đề nghị tập trung vào phát triển các từ điển, điều này đã loại bỏ các nhà nghiên cứu Mỹ ra khỏi cuộc đua trong gần một thập kỷ.

%Mặc dù vậy, cơ sở cho việc Xử lý ngôn ngữ tự nhiên hiện đại đã được tạo ra chỉ bởi các nhà khoa học và nỗ lực, nghiên cứu và phát triển của họ. 

\section{Các phương pháp Dịch máy}
Từ đó đến nay, đã có nhiều hướng tiếp cập đã được sử dụng trong dịch máy với mục tiêu tạo ra bản dịch có độ chính xác cao và giảm thiểu công sức của con người có thể chia làm hai nhóm. Nhóm đầu tiên là những hướng tiếp cận dựa trên \textit{Từ điển} (Dictionary based). Đây là hướng tiếp cận chính cho những nghiên cứu về Dịch máy trong những năm 1950-1960. Những phương pháp dựa trên hướng tiếp cập Từ điển có thể kể đến như \textit{Dịch trực tiếp} (Direct machine translation), \textit{Dịch máy chuyển dịch} (Transfer-based machine translation) hay \textit{Dịch máy ngôn ngữ đại diện} (Interlingual machine translation). Điểm chung của những phương pháp này là dùng một từ điển để dịch các từ từ ngôn ngữ nguồn sang ngôn ngữ đích và sau đó cố gắng chỉnh sửa bản dịch để tạo ra một câu có nghĩa. Nhóm phương pháp này thường yêu cầu một bộ từ điển giữa hai ngôn ngữ cần dịch và một tập các quy tắc ngữ pháp cho mỗi ngôn ngữ. Bản dịch của hướng tiếp cận Từ điển thường có chất lượng kém và không sử dụng được trừ một số trường hợp đặc biệt. Ngoài ra chúng còn đòi hỏi một lượng nhân lực lớn với hiểu biết sâu sắc cho việc xây dựng những bộ từ điển và các quy tắc ngôn ngữ. 

Nhóm thứ hai là những hướng tiếp cận dựa trên \textit{Ngữ liệu} (Corpus based). Nhóm này hoạt động dựa trên một tập dữ liệu song song của các cặp câu là bản dịch của nhau trong hai ngôn ngữ gọi là Ngữ liệu và chỉ yêu cầu những tri thức tối thiểu về ngôn ngữ học.


%Từ đó đến nay, đã có nhiều hướng tiếp cập đã được sử dụng trong dịch máy với mục tiêu tạo ra bản dịch có độ chính xác cao và giảm thiểu công sức của con người bao gồm nhóm Dịch máy dựa trên luật, Dịch máy dựa trên ví dụ, Dịch máy thống kê hay gần đây nhất là Dịch máy Nơ-ron (hình 1.1). Mặc dù được nghiên cứu sôi nổi trong thời kì bắt đầu những năm 1950-1960, những nghiên cứu thời đó thường xoay quanh nhóm phương pháp \textit{Dịch máy dựa trên luật} (Rule based machine translation). Dịch máy dựa trên luật dùng một từ điển để dịch các từ từ ngôn ngữ nguồn sang ngôn ngữ đích và sau đó cố gắng chỉnh sửa bản dịch để tạo ra một câu có nghĩa. Nhóm phương pháp này thường yêu cầu một bộ từ điển giữa hai ngôn ngữ cần dịch và một tập các quy tắc ngữ pháp cho mỗi ngôn ngữ. Bản dịch của Dịch máy dựa trên luật thường có chất lượng kém và không sử dụng được trừ một số trường hợp đặc biệt, ngoài ra nó còn đòi hỏi một lượng tài nguyên rất lớn cho việc xây dựng những bộ từ điển và các quy tắc ngôn ngữ. Thêm vào đó, những từ đồng nghĩa và tính nhập nhằng của ngôn ngữ cũng làm giảm chất lượng của phương pháp này.
%\begin{figure}
%	\centering
%	\includegraphics[width=\textwidth]{rulebased}
%	\caption[Minh họa Dịch máy dựa trên luật]{Minh họa Dịch máy dựa trên luật}
%	\label{fig_rulebasedmt}
%\end{figure}

\section{Dịch máy Nơ-ron}

Mặc dù trên thực tế đã có nhiều hệ thống Dịch máy được phát triển dựa trên Dịch máy Thống kê thời bấy giờ, tuy nhiên nó không hoạt động thực sự tốt bởi một số nguyên nhân. Một trong số đó là việc những từ hay đoạn được dịch cục bộ và quan hệ của chúng với những từ cách xa chúng trong câu nguồn thường bị bỏ qua. Bên cạnh đó, mô hình ngôn ngữ N-gram hoạt động không thực sự tốt đối với những bản dịch dài và ta phải tốn nhiều bộ nhớ để lưu trữ chúng. Ngoài ra việc sử dụng nhiều thành phần nhỏ được điều chỉnh riêng biệt như mô hình dịch, mô hình ngôn ngữ, gióng hàng,.. cũng gây khó khăn cho việc vận hành và phát triển mô hình này.

\textit{Dịch máy Nơ-ron} (Neural machine translation) là một hướng tiếp cận mới trong dịch máy trong những năm gần đây được đề xuất đầu tiên bởi Kalchbrenner and Blunsom (2013), Sutskever et al. (2014) and Cho et al. (2014b). Giống như Dịch máy thống kê, Dịch máy Nơ-ron cũng là một phương pháp thuộc hướng tiếp cận dựa trên Ngữ liệu, trong khi Dịch máy Thống kê bao gồm nhiều mô-đun nhỏ được điều chỉnh riêng biệt, Dịch máy Nơ-ron cố gắng dùng một mạng Nơ-ron như là thành phần duy nhất của hệ thống, mọi thiết lập sẽ được thực hiện trên mạng này. 

Hầu hết những mô hình Dịch máy Nơ-ron đều dựa trên kiến trúc \textit{Bộ mã hóa - bộ giải mã} (encoder-decoder) (Sutskever et al., 2014; Cho et al., 2014a). Bộ mã hóa thường là một mạng Nơ-ron có tác dụng \textit{"nén"} tất cả thông tin của câu trong ngôn ngữ nguồn vào một vector có kích thước cố định. Bộ giải mã, cũng là một mạng Nơ-ron, sẽ tạo bản dịch trong ngôn ngữ đích từ vector có kích thước cố định kia. Toàn bộ hệ thống bao gồm bộ mã hóa và bộ giải mã sẽ được huấn luyện \textit{"end-to-end"} để tạo ra bản dịch, quá trình này được mô tả như hình 1.2.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{intro2nmt}
	\caption[Ví dụ về Kiến trúc \textit{Bộ mã hóa - bộ giải mã} trong dịch máy Nơ-ron]{Ví dụ về Kiến trúc Bộ mã hóa - bộ giải mã trong dịch máy Nơ-ron}
	\label{fig_intronmt}
\end{figure}

Trong thực tế cả Bộ mã hóa và giải mã thường dựa trên một mô hình mạng nơ-ron tên là \textit{Mạng nơ-ron hồi quy} là một thiết kế mạng đặc trưng cho việc xử lý dữ liệu chuỗi. Mạng nơ-ron hồi quy cho phép chúng ta mô hình hóa những dữ liệu có độ dài không xác định, rất thích hợp cho bài toán dịch máy. Hình 1.3 mô tả chi tiết hơn về kiến trúc Bộ mã hóa - giải mã sử dụng Mạng nơ-ron hồi quy. Đầu tiên Bộ mã hóa đọc qua toàn bộ câu nguồn và tạo ra một vector đại diện gọi là \textit{vector trạng thái}. Điều này giúp cho toàn bộ những thông tin cần thiết hay quan hệ giữa những từ cách xa nhau đều được tập hợp vào một nơi duy nhất. Bộ giải mã, lúc này đóng vai trò như một Mô hình ngôn ngữ để tạo ra từng từ trong ngôn ngữ đích và sẽ dừng lại đến khi một ký tự đặc biệt xuất hiện.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{intro2nmt}
	\caption[Ví dụ về Kiến trúc \textit{Bộ mã hóa - bộ giải mã} trong dịch máy Nơ-ron]{Ví dụ về Kiến trúc Bộ mã hóa - bộ giải mã trong dịch máy Nơ-ron}
	\label{fig_intronmt}
\end{figure}


%Trong những năm gần đây, phương pháp học có giám sát của máy học đã được áp dụng để giải quyết nhiều bài toán trong thị giác máy tính, xử lý âm thanh, xử lý ngôn ngữ tự nhiên, … Thông thường, từ dạng biểu diễn thô ban đầu (ví dụ như ma trận pixel), ta cần phải chuyển sang một dạng biểu diễn tốt hơn, có nhiều ngữ nghĩa hơn – gọi là \emph{biểu diễn đặc trưng} (feature representation); rồi mới đưa dạng biểu diễn đặc trưng này vào một thuật toán học có giám sát (ví dụ như SVM). Việc xác định cách biểu diễn đặc trưng đóng vai trò rất quan trọng để thuật toán học giám sát có thể thực hiện hiệu quả. 

%Để xác định cách biểu diễn đặc trưng, hướng tiếp cận truyền thống là \emph{thiết kế đặc trưng một cách thủ công} (hand-designed features). Nghĩa là với một lĩnh vực cụ thể, sẽ có những nhóm nghiên cứu gồm các chuyên gia trong lĩnh vực đó cùng ngồi xuống, phân tích, thiết kế, ``thử và sai'' các cách biểu diễn đặc trưng từ dạng biểu diễn thô ban đầu. Ví dụ về các đặc trưng được thiết kế thủ công trong thị giác máy tính là SIFT, HOG, SURF, …; trong xử lý âm thanh là Spectrogram, MFCC, Spectral rolloff, … Tuy nhiên, nhược điểm của hướng tiếp cận này là tốn thời gian và tốn sức lao động; đồng thời cũng thiếu tính tổng quát hóa: các đặc trưng này chỉ sử dụng được cho một loại dữ liệu cụ thể (ví dụ, đặc trưng SIFT chỉ sử dụng được cho một số loại ảnh cụ thể trong thị giác máy tính). Ngoài ra, việc thiết kế đặc trưng một cách thủ công như trên cũng cho thấy điểm yếu của các thuật toán máy học hiện nay: thiếu khả năng tự động rút trích các thông tin có ích trực tiếp từ dữ liệu thô ban đầu.

%Do đó, thay vì thiết kế các trưng một cách thủ công, ta mong muốn có một thuật toán có thể \emph{tự động học các đặc trưng} từ dữ liệu thô ban đầu. Hơn nữa, ta mong muốn tìm được một thuật toán mà có thể áp dụng tổng quát cho nhiều loại dữ liệu (hình ảnh, âm thanh, …). Ngoài ra, ta muốn học các đặc trưng từ tập dữ liệu không có nhãn (unlabeled data) vì dữ liệu không có nhãn có rất nhiều; trong khi đó, dữ liệu có nhãn không có nhiều và phải tốn chi phí để có thể có thêm. Hướng nghiên cứu này được gọi là \emph{học đặc trưng không giám sát} (unsupervised feature learning). Đây là một hướng nghiên cứu mới trong máy học và đang thu hút được rất nhiều sự quan tâm trong thời gian gần đây.

%Để giải quyết bài toán học đặc trưng không giám sát, câu hỏi lớn và mang tính định hướng nghiên cứu dài hạn là: \emph{Thế nào là một biểu diễn đặc trưng tốt?} Theo GS. Yoshua Bengio, một trong những nhà nghiên cứu tiên phong trong lĩnh vực học biểu diễn đặc trưng, thì: \emph{Một biễu diễn đặc trưng tốt cần \textbf{phân tách (disentangle)} được các yếu tố giải thích ẩn bên dưới} \cite{bengio2013representation}. Hình \ref{fig_disentangle} minh họa cho điểm này; ở đây, chó, mèo, cây, ... là các yếu tố giải thích cho bức ảnh và một biểu diễn đặc trưng tốt cần tìm ra được các yếu tố giải thích này. 


%\begin{figure}
%	\centering
%	\includegraphics[width=\textwidth]{disentangle}
%	\caption[Minh họa về một biểu diễn đặc trưng tốt]{Minh họa về một biểu diễn đặc trưng tốt: phân tách được các yếu tố giải thích ẩn bên dưới (chó, mèo, cây, ...)}
%	\label{fig_disentangle}
%\end{figure}

%Cho đến nay, vẫn chưa có một thuật toán học đặc trưng mà có thể phân tách tốt được các yếu tố giải thích ẩn bên dưới. Để có thể phân tách được các yếu tố giải thích ẩn, ta cần có \emph{các sự hiểu biết trước (priors)} về các yếu tố này. Ở đây, ta quan tâm đến các sự hiểu biết trước mang tính tổng quát, có thể áp dụng để học đặc trưng trong nhiều bài toán liên quan đến trí tuệ nhân tạo (thị giác máy tính, xử lý ngôn ngữ tự nhiên, ...). Dưới đây là một số sự hiểu biết trước như vậy \cite{bengio2013representation}:
%\begin{itemize}
%	\item \textbf{Các yếu tố giải thích được tổ chức một cách phân cấp}: Thế giới xung quanh ta có thể được mô tả bằng một kiến trúc phân cấp. Cụ thể là, các yếu tố hay các khái niệm (concept) trừu tượng (ví dụ như con mèo, cái cây, ...) bao gồm các khái niệm ít trừu tượng hơn; các khái niệm ít trừu tượng hơn này lại bao gồm các khái niệm ít trừu tượng hơn nữa ... \emph{Học sâu (deep learning)} sử dụng sự hiểu biết này: học nhiều tầng biểu diễn đặc trưng với độ trừu tượng tăng dần.
%	\item \textbf{Gom cụm tự nhiên (natural clustering)}: các mẫu thuộc các lớp khác nhau nằm trên các đa tạp (manifold) khác nhau và các đa tạp này được phân tách tốt với nhau bởi các vùng có mật độ thấp; hơn nữa, số chiều của các đa tạp này nhỏ hơn rất nhiều so với số chiều của không gian ban đầu.
%	\item \textbf{Tính thưa (sparsity)}: với mỗi mẫu dữ liệu (ví dụ, với mỗi bức ảnh), chỉ có một số ít các khái niệm (hay các yếu tố giải thích) trong tập các khái niệm. Do đó, với mỗi mẫu dữ liệu, ta muốn tìm một véc-tơ biểu diễn \emph{thưa}, nghĩa là hầu hết các phần tử của véc-tơ này có giá trị bằng 0 (ứng với các khái niệm không liên quan trong tập khái niệm).
%\end{itemize}

%Trong luận văn này, chúng tôi sẽ tập trung nghiên cứu về tính thưa. Việc tích hợp tất cả các hiểu biết trước ở trên (cũng như là tìm ra thêm các hiểu biết trước mới) vào trong cùng một mô hình sẽ có thể giúp phân tách các yếu tố giải thích ẩn tốt hơn, nhưng đây là một điều không đơn giản và chúng tôi để lại như là một định hướng cho việc nghiên cứu trong tương lai. Tính thưa lần đầu tiên được đề xuất trong thuật toán ``Sparse Coding'' \cite{olshausen1996emergence} để mô hình vùng vỏ não thị giác V1 (là vùng đầu tiên xử lý tín hiệu thị giác từ võng mạc mắt). Và điểm thú vị là ``Sparse Coding'' có thể học được những đặc trưng tương tự như những đặc trưng của vùng V1 (có dạng các cạnh ở các vị trí khác nhau và với các hướng khác nhau).

%``Sparse Auto-Encoders'' (SAEs) có thể học được các đặc trưng giống với ``Sparse Coding''. Tuy nhiên, so với ``Sparse Coding'', SAEs có những điểm lợi như sau:
%\begin{itemize}
%	\item Việc huấn luyện SAEs có thể được thực hiện một cách hiệu quả với thuật toán lan truyền ngược (back-propagation).
%	\item Sau khi đã được huấn luyện, với một véc-tơ đầu vào mới, SAEs có thể tính ra véc-tơ đặc trưng tương ứng rất nhanh; trong khi đó, ``Sparse Coding'' vẫn phải tiến hành tối ưu hóa.
%	\item Sau khi đã học đặc trưng không giám sát, SAEs có thể cho phép điều chỉnh lại (fine-tune) các đặc trưng này với các mẫu huấn luyện có nhãn. Nhìn chung, việc điều chỉnh này thường sẽ cho kết quả tốt hơn so với việc không điều chỉnh.
%\end{itemize}

%Tuy có những lợi điểm trên, nhưng trong thực tế thì không dễ để làm cho SAEs ``hoạt động''. Để làm cho SAEs ``hoạt động'', có hai điểm cần phải làm rõ: (i) ràng buộc thưa, và (ii) ràng buộc trọng số. Sử dụng chuẩn L1 để ràng buộc tính thưa của véc-tơ đặc trưng là một cách tự nhiên (vì L1 được dùng trong ``Sparse Coding'') và đơn giản (trong trường hợp véc-tơ đặc trưng có giá trị dương, L1 đơn giản là bằng tổng của các phần tử của véc-tơ này), nhưng L1 lại thường không được dùng trong SAEs với lý do vẫn còn chưa rõ ràng \cite{bengio2013representation}. Thay vì dùng L1, các nghiên cứu liên quan đến SAEs thường ràng buộc thưa bằng cách ép giá trị đầu ra trung bình của mỗi nơ-ron ẩn (trong SAEs, mỗi nơ-ron ẩn ứng với một đặc trưng) về một giá trị cố định gần 0 \cite{goodfellow2009measuring}\cite{coates2011analysis}\cite{coates2012demystifying}. Tuy nhiên, giá trị cố định này lại thêm một siêu tham số (hyper-parameter, là tham số mà ta phải chọn trước khi huấn luyện) vào danh sách các siêu tham số vốn đã rất nhiều của SAEs; điều này sẽ làm cho quá trình chọn lựa các siêu tham số trở nên rất ``phiền phức'' và tốn thời gian. Về vấn đề ràng buộc trọng số của SAEs, có một số cách khác nhau đã được sử dụng. \cite{coates2012demystifying} ràng buộc các trọng số của bộ mã hóa (encoder) giống với các trọng số của bộ giải mã (decoder). Cách ràng buộc trọng số này cũng được dùng cho các loại ``Auto-Encoders'' khác như ``Denoising Auto-Encoders'' \cite{vincent2008extracting} và ``Contractive Auto-Encoders'' \cite{rifai2011contractive}\cite{rifai2011HCAEs}. \cite{goodfellow2009measuring}\cite{coates2011analysis} dùng một cách ràng buộc trọng số khác là ``weight decay'' (phạt tổng bình phương các trọng số); cách này lại làm xuất hiện thêm một siêu tham số nữa. \cite{zeiler2013rectified} ràng buộc các véc-tơ trọng số của tầng giải mã (mỗi véc-tơ ứng với các trọng số đi ra ở mỗi nơ-ron ẩn) có độ dài bằng một. Tuy nhiên, trong số các cách ràng buộc trọng số này, không rõ là nên sử dụng cách nào cũng như là tại sao nên ràng buộc các trọng số như vậy.

%Như vậy, có hai câu hỏi cần phải được trả lời: (i) tại sao chuẩn L1 lại thường không được dùng để ràng buộc thưa trong SAEs?; (ii) liệu có cách nào tốt hơn và hợp lý hơn để ràng buộc trọng số của SAEs không? Trong luận văn này, chúng tôi sẽ cố gắng trả lời hai câu hỏi này. Cụ thể là:
%\begin{itemize}
%	\item Chúng tôi cố gắng hiểu khó khăn của việc huấn luyện SAE với ràng buộc thưa dùng chuẩn L1. Từ đó, chúng tôi đề xuất một phiên bản điều chỉnh của thuật toán tối ưu hóa ``Stochastic Gradient Descent'' (SGD), gọi là ``Sleep-Wake Stochastic Gradient Descent'' (SW-SGD), để giải quyết khó khăn này. Ở đây, chúng tôi tập trung nghiên cứu SAEs với hàm kích hoạt ở tầng ẩn là hàm ``rectified linear'' ($f(x) = \max(0, x)$) bởi vì hàm này tính nhanh và cho tính thưa thật sự (đúng bằng 0). Chúng tôi gọi SAEs với hàm kích hoạt này là \emph{``Sparse Rectified Auto-Encoders''} (SRAEs).
%	\item Hơn nữa, chúng tôi cũng đề xuất một cách hợp lý để ràng buộc trọng số của SRAEs.
%\end{itemize}

%Với hai thành phần trên (SW-SGD và cách ràng buộc trọng số mà chúng tôi đề xuất), kết quả thí nghiệm trên bộ dữ liệu MNIST (bộ dữ liệu chữ số viết tay từ 0 đến 9) cho thấy SRAEs có thể học được những đặc trưng có ích và những đặc trưng này cho kết quả phân lớp tốt khi so sánh với các loại ``Auto-Encoders'' khác.

%Phần còn lại của luận văn được trình bày như sau:
%\begin{itemize}
%	\item Chương 2 trình bày kiến thức nền tảng về ``Sparse Coding'' và ``Sparse Auto-Encoders''.
%	\item Chương 3 trình bày về ``Sparse Rectified Auto-Encoders'' (SRAEs); đây là phần chính của luận văn. Trong phần này gồm có hai phần nhỏ:
%	\begin{itemize}
%		\item Ràng buộc thưa: chúng tôi giải thích về vấn đề gặp phải khi huấn luyện SRAEs với chuẩn L1 và đưa ra giải pháp để giải quyết vấn đề này.
%		\item Ràng buộc trọng số: chúng tôi trình bày về cách ràng buộc trọng số đề xuất cho SRAEs.
%	\end{itemize}
%	\item Chương 4 trình bày về các thí nghiệm và các phân tích về kết quả đạt được.
%	\item Cuối cùng, kết luận và hướng phát triển được trình bày ở chương 5.
%\end{itemize}